# üîó Guide d'Int√©gration Frontend ‚ÜîÔ∏è Backend

## üìê Architecture Compl√®te

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Projet Complet                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Frontend (React)    ‚îÇ         ‚îÇ  Backend (FastAPI)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Port: 5173          ‚îÇ  HTTP   ‚îÇ  Port: 8000          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - UI/UX             ‚îÇ         ‚îÇ  - ML Pipeline       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - React Query       ‚îÇ         ‚îÇ  - RAG System        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - TailwindCSS       ‚îÇ         ‚îÇ  - FAISS Search      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                              ‚îÇ                ‚îÇ
‚îÇ                                              ‚ñº                ‚îÇ
‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ                                    ‚îÇ  Mod√®les ML          ‚îÇ  ‚îÇ
‚îÇ                                    ‚îÇ  - LLaMA 3.2 3B      ‚îÇ  ‚îÇ
‚îÇ                                    ‚îÇ  - BAAI/bge          ‚îÇ  ‚îÇ
‚îÇ                                    ‚îÇ  - FAISS Index       ‚îÇ  ‚îÇ
‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìç O√π se trouvent les mod√®les ?

### ü¶ô LLaMA 3.2 3B et BAAI/bge (HuggingFace)

Les mod√®les sont **automatiquement t√©l√©charg√©s** lors de la premi√®re utilisation et stock√©s dans :

**Windows :**
```
C:\Users\hp\.cache\huggingface\hub\
```

**Contenu du cache :**
```
huggingface/
‚îî‚îÄ‚îÄ hub/
    ‚îú‚îÄ‚îÄ models--meta-llama--Llama-3.2-3B-Instruct/
    ‚îÇ   ‚îî‚îÄ‚îÄ (fichiers du mod√®le LLaMA ~6GB avec quantification)
    ‚îÇ
    ‚îî‚îÄ‚îÄ models--BAAI--bge-small-en-v1.5/
        ‚îî‚îÄ‚îÄ (fichiers du mod√®le embeddings ~130MB)
```

**Taille totale :** ~2-3 GB pour LLaMA quantifi√© + ~130 MB pour BAAI = **~3 GB**

### üìä Index FAISS (vos donn√©es)

Stock√© dans votre projet :
```
d:\iso-doc-navigator-main\ml_core\data\index\
‚îú‚îÄ‚îÄ faiss_index.bin           (vecteurs)
‚îú‚îÄ‚îÄ faiss_index_metadata.json (m√©tadonn√©es)
‚îî‚îÄ‚îÄ faiss_index_config.json   (configuration)
```

---

## üöÄ Comment lancer le projet complet

### Option 1 : D√©veloppement (2 terminaux)

**Terminal 1 - Backend ML Core :**
```bash
# Aller dans le dossier backend
cd d:\iso-doc-navigator-main\ml_core

# Installer les d√©pendances (premi√®re fois)
pip install -r requirements.txt

# Lancer l'API
uvicorn ml_core.api.api:app --reload --host 0.0.0.0 --port 8000
```
‚úÖ Backend disponible sur : http://localhost:8000  
üìö Documentation API : http://localhost:8000/docs

**Terminal 2 - Frontend React :**
```bash
# Aller dans le dossier frontend
cd d:\iso-doc-navigator-main\iso-doc-navigator-main

# Installer les d√©pendances (premi√®re fois)
npm install

# Cr√©er le fichier .env.local
copy .env.example .env.local
# Puis √©diter .env.local et mettre : VITE_ML_API_URL=http://localhost:8000

# Lancer le frontend
npm run dev
```
‚úÖ Frontend disponible sur : http://localhost:5173

---

### Option 2 : Docker (tout-en-un)

Je vais cr√©er un docker-compose qui lance les deux services ensemble :

```bash
# √Ä la racine du projet
docker-compose up -d

# Frontend : http://localhost:5173
# Backend : http://localhost:8000
```

---

## üîå Utilisation de l'API dans le Frontend

### Exemple : Poser une question

```typescript
import { useAskQuestion } from '@/hooks/useMLCore';

function QuestionComponent() {
  const askQuestion = useAskQuestion();

  const handleSubmit = async (question: string) => {
    const result = await askQuestion.mutateAsync({
      query: question,
      top_k: 5,
      temperature: 0.7
    });

    console.log('Answer:', result.answer);
    console.log('Sources:', result.sources);
  };

  return (
    <div>
      <input onChange={(e) => handleSubmit(e.target.value)} />
      {askQuestion.isPending && <p>Loading...</p>}
      {askQuestion.data && (
        <div>
          <p>{askQuestion.data.answer}</p>
          <ul>
            {askQuestion.data.sources.map(source => (
              <li key={source.chunk_id}>
                {source.section} - {source.section_name} (page {source.page})
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}
```

### Exemple : V√©rifier le statut du backend

```typescript
import { useSystemInfo } from '@/hooks/useMLCore';

function StatusIndicator() {
  const { data, isLoading } = useSystemInfo();

  if (isLoading) return <p>Checking...</p>;

  return (
    <div>
      <p>Status: {data?.status}</p>
      <p>Model: {data?.model}</p>
      <p>Index Size: {data?.index_size} chunks</p>
    </div>
  );
}
```

---

## üì¶ Fichiers cr√©√©s pour l'int√©gration

1. ‚úÖ **`.env.example`** - Configuration exemple
2. ‚úÖ **`src/services/mlCoreAPI.ts`** - Client API TypeScript
3. ‚úÖ **`src/hooks/useMLCore.ts`** - React hooks

### Pour les utiliser :

1. **Cr√©er `.env.local` √† partir de `.env.example`**
   ```bash
   cd iso-doc-navigator-main
   copy .env.example .env.local
   ```

2. **√âditer `.env.local`** et confirmer :
   ```
   VITE_ML_API_URL=http://localhost:8000
   ```

3. **Importer dans vos composants React** :
   ```typescript
   import { useAskQuestion, useSystemInfo } from '@/hooks/useMLCore';
   ```

---

## üîß Configuration Compl√®te

### Frontend (package.json - d√©j√† install√©)
- React + TypeScript ‚úÖ
- Vite ‚úÖ
- TailwindCSS + shadcn/ui ‚úÖ
- React Router ‚úÖ
- React Query (@tanstack/react-query) ‚úÖ

### Backend (ml_core/requirements.txt)
- FastAPI ‚úÖ
- Transformers (HuggingFace) ‚úÖ
- FAISS ‚úÖ
- Sentence Transformers ‚úÖ

---

## üìù Script de lancement complet

Cr√©ez `start.bat` √† la racine :

```batch
@echo off
echo ========================================
echo  Starting ISO Document Navigator
echo ========================================

echo.
echo [1/2] Starting ML Core Backend...
start cmd /k "cd ml_core && uvicorn ml_core.api.api:app --reload --port 8000"

timeout /t 5

echo.
echo [2/2] Starting Frontend...
start cmd /k "cd iso-doc-navigator-main && npm run dev"

echo.
echo ========================================
echo  Project Started!
echo ========================================
echo  Frontend: http://localhost:5173
echo  Backend:  http://localhost:8000/docs
echo ========================================
```

Puis double-cliquez sur `start.bat` pour tout lancer !

---

## ‚öôÔ∏è T√©l√©chargement des mod√®les

### Premi√®re ex√©cution

Lors du premier lancement, les mod√®les HuggingFace seront t√©l√©charg√©s automatiquement :

```bash
# Quand vous lancez le backend la premi√®re fois :
uvicorn ml_core.api.api:app --reload

# Vous verrez :
Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà| 899/899
Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà| 2.87G/2.87G
```

**Temps estim√©** : 10-20 minutes (selon connexion internet)  
**Espace requis** : ~3 GB

### Forcer le pr√©-t√©l√©chargement (optionnel)

```python
# Dans ml_core/
python -c "
from ml_core.models.llama_loader import LLaMALoader
from ml_core.embeddings.embedder import Embedder

print('Downloading LLaMA...')
loader = LLaMALoader(model_name='llama-3.2-3b', quantize=True)
loader.load_model()

print('Downloading BAAI embeddings...')
embedder = Embedder()

print('‚úÖ All models downloaded!')
"
```

---

## ‚úÖ Checklist de d√©marrage

- [ ] Backend install√© : `cd ml_core && pip install -r requirements.txt`
- [ ] Frontend install√© : `cd iso-doc-navigator-main && npm install`
- [ ] `.env.local` cr√©√© avec `VITE_ML_API_URL=http://localhost:8000`
- [ ] PDFs plac√©s dans `ml_core/data/pdfs/`
- [ ] Backend lanc√© : port 8000 ‚úÖ
- [ ] Frontend lanc√© : port 5173 ‚úÖ
- [ ] Test : http://localhost:8000/docs (Swagger UI)
- [ ] Test : http://localhost:5173 (Interface React)

---

## üéØ Workflow complet

1. **Lancer les deux serveurs** (backend + frontend)
2. **Ing√©rer vos PDFs ISO** via l'API `/ingest`
3. **Poser des questions** via l'interface React
4. **Le syst√®me :**
   - Recherche dans l'index FAISS
   - G√©n√®re une r√©ponse avec LLaMA
   - Renvoie l'answer + sources au frontend

**üöÄ Votre projet est maintenant 100% fonctionnel !**
